{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled17.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMsVNf42rP9KXkbK/F79eOY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sanjay151102/COD-Lab/blob/main/Untitled17.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6VbvDAQs2N3i",
        "outputId": "57b5f5ce-d480-414f-eaef-5a9fadb50a58"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorSpec(shape=(), dtype=tf.string, name=None),\n",
              " TensorSpec(shape=(), dtype=tf.int64, name=None))"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow as tf\n",
        "\n",
        "tfds.disable_progress_bar()\n",
        "train_dataset, test_dataset = dataset['train'], dataset['test']\n",
        "\n",
        "train_dataset.element_spec"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lrJS3i8g5-Yc",
        "outputId": "612864ad-f847-4a58-8017-2e1b2e76cf3a"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<PrefetchDataset element_spec=(TensorSpec(shape=(), dtype=tf.string, name=None), TensorSpec(shape=(), dtype=tf.int64, name=None))>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "x86Ef6Ep6SJA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = dataset, info = tfds.load('imdb_reviews', with_info=True,\n",
        "                          as_supervised=True)"
      ],
      "metadata": {
        "id": "UrmVW1042Ryt"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GBdHrdJa2f6l",
        "outputId": "e33dc329-bd7f-4228-ff56-9fc784daa9ca"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "({'train': <PrefetchDataset element_spec=(TensorSpec(shape=(), dtype=tf.string, name=None), TensorSpec(shape=(), dtype=tf.int64, name=None))>, 'test': <PrefetchDataset element_spec=(TensorSpec(shape=(), dtype=tf.string, name=None), TensorSpec(shape=(), dtype=tf.int64, name=None))>, 'unsupervised': <PrefetchDataset element_spec=(TensorSpec(shape=(), dtype=tf.string, name=None), TensorSpec(shape=(), dtype=tf.int64, name=None))>}, tfds.core.DatasetInfo(\n",
            "    name='imdb_reviews',\n",
            "    full_name='imdb_reviews/plain_text/1.0.0',\n",
            "    description=\"\"\"\n",
            "    Large Movie Review Dataset.\n",
            "    This is a dataset for binary sentiment classification containing substantially more data than previous benchmark datasets. We provide a set of 25,000 highly polar movie reviews for training, and 25,000 for testing. There is additional unlabeled data for use as well.\n",
            "    \"\"\",\n",
            "    config_description=\"\"\"\n",
            "    Plain text\n",
            "    \"\"\",\n",
            "    homepage='http://ai.stanford.edu/~amaas/data/sentiment/',\n",
            "    data_path='~/tensorflow_datasets/imdb_reviews/plain_text/1.0.0',\n",
            "    file_format=tfrecord,\n",
            "    download_size=80.23 MiB,\n",
            "    dataset_size=129.83 MiB,\n",
            "    features=FeaturesDict({\n",
            "        'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=2),\n",
            "        'text': Text(shape=(), dtype=tf.string),\n",
            "    }),\n",
            "    supervised_keys=('text', 'label'),\n",
            "    disable_shuffling=False,\n",
            "    splits={\n",
            "        'test': <SplitInfo num_examples=25000, num_shards=1>,\n",
            "        'train': <SplitInfo num_examples=25000, num_shards=1>,\n",
            "        'unsupervised': <SplitInfo num_examples=50000, num_shards=1>,\n",
            "    },\n",
            "    citation=\"\"\"@InProceedings{maas-EtAl:2011:ACL-HLT2011,\n",
            "      author    = {Maas, Andrew L.  and  Daly, Raymond E.  and  Pham, Peter T.  and  Huang, Dan  and  Ng, Andrew Y.  and  Potts, Christopher},\n",
            "      title     = {Learning Word Vectors for Sentiment Analysis},\n",
            "      booktitle = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies},\n",
            "      month     = {June},\n",
            "      year      = {2011},\n",
            "      address   = {Portland, Oregon, USA},\n",
            "      publisher = {Association for Computational Linguistics},\n",
            "      pages     = {142--150},\n",
            "      url       = {http://www.aclweb.org/anthology/P11-1015}\n",
            "    }\"\"\",\n",
            "))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Text\n",
        "for text, label in train_dataset.take(1):\n",
        "  print('text: ', text.numpy())\n",
        "  print('label: ', label.numpy())\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R0mZQ8eg5vAl",
        "outputId": "56b0cd50-bf20-44a5-9e63-a5f40c407332"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "text:  b\"This was an absolutely terrible movie. Don't be lured in by Christopher Walken or Michael Ironside. Both are great actors, but this must simply be their worst role in history. Even their great acting could not redeem this movie's ridiculous storyline. This movie is an early nineties US propaganda piece. The most pathetic scenes were those when the Columbian rebels were making their cases for revolutions. Maria Conchita Alonso appeared phony, and her pseudo-love affair with Walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning. I am disappointed that there are movies like this, ruining actor's like Christopher Walken's good name. I could barely sit through it.\"\n",
            "label:  0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_spaces(text):\n",
        "    text = re.sub(r\" '(\\w)\",r\"'\\1\",text)\n",
        "    text = re.sub(r\" \\,\",\",\",text)\n",
        "    text = re.sub(r\" \\.+\",\".\",text)\n",
        "    text = re.sub(r\" \\!+\",\"!\",text)\n",
        "    text = re.sub(r\" \\?+\",\"?\",text)\n",
        "    text = re.sub(\" n't\",\"n't\",text)\n",
        "    text = re.sub(\"[\\(\\)\\;\\_\\^\\`\\/]\",\"\",text)\n",
        "    \n",
        "    return text"
      ],
      "metadata": {
        "id": "CPJFCju26rno"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def decontract(text):\n",
        "    text = re.sub(r\"won\\'t\", \"will not\", text)\n",
        "    text = re.sub(r\"can\\'t\", \"can not\", text)\n",
        "    text = re.sub(r\"n\\'t\", \" not\", text)\n",
        "    text = re.sub(r\"\\'re\", \" are\", text)\n",
        "    text = re.sub(r\"\\'s\", \" is\", text)\n",
        "    text = re.sub(r\"\\'d\", \" would\", text)\n",
        "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
        "    text = re.sub(r\"\\'t\", \" not\", text)\n",
        "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
        "    text = re.sub(r\"\\'m\", \" am\", text)\n",
        "    return text"
      ],
      "metadata": {
        "id": "Lofn6DNu8hW2"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(text):\n",
        "    text = re.sub(\"\\n\",\"\",text) \n",
        "    text = remove_spaces(text)   \n",
        "    text = re.sub(r\"\\.+\",\".\",text) \n",
        "    text = re.sub(r\"\\!+\",\"!\",text)\n",
        "    text = decontract(text)    \n",
        "    text = re.sub(\"[^A-Za-z0-9 ]+\",\"\",text) \n",
        "    text = text.lower() \n",
        "    return text"
      ],
      "metadata": {
        "id": "9NVGpE7r8ppm"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Dataset :\n",
        "    def __init__(self, data , tk_inp ,tk_out, max_len):\n",
        "        self.encoder_inp = data[\"enc_input\"].apply(str).values\n",
        "        self.decoder_inp = data[\"dec_input\"].apply(str).values\n",
        "        self.decoder_out = data[\"dec_output\"].apply(str).values\n",
        "        self.tk_inp = tk_inp\n",
        "        self.tk_out = tk_out\n",
        "        self.max_len = max_len\n",
        "    def __getitem__(self,i):\n",
        "        self.encoder_seq = self.tk_inp.texts_to_sequences([self.encoder_inp[i]])\n",
        "        self.decoder_inp_seq = self.tk_out.texts_to_sequences([self.decoder_inp[i]])\n",
        "        self.decoder_out_seq = self.tk_out.texts_to_sequences([self.decoder_out[i]])\n",
        "        self.encoder_seq = pad_sequences(self.encoder_seq, padding=\"post\",maxlen = self.max_len)\n",
        "        self.decoder_inp_seq = pad_sequences(self.decoder_inp_seq, padding=\"post\",maxlen = self.max_len)\n",
        "        self.decoder_out_seq = pad_sequences(self.decoder_out_seq ,padding=\"post\", maxlen = self.max_len)\n",
        "        return self.encoder_seq ,  self.decoder_inp_seq,  self.decoder_out_seq\n",
        "    def __len__(self):\n",
        "        # RETURN THE LEN OF INPUT ENDODER\n",
        "        return len(self.encoder_inp)\n",
        "    "
      ],
      "metadata": {
        "id": "xp2oLdPp8_Hm"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Dataloader(tf.keras.utils.Sequence):\n",
        "    def __init__(self,batch_size,dataset):\n",
        "        # INTIALIZING THE REQUIRED VARIABLES \n",
        "        self.dataset = dataset\n",
        "        self.batch_size = batch_size\n",
        "        self.totl_points = self.dataset.encoder_inp.shape[0]\n",
        "        \n",
        "    def __getitem__(self,i):\n",
        "        # STATING THE START AND STOP VATIABLE CONTAINGING INDEX VALUES FOR EACH BATCH\n",
        "        start = i * self.batch_size\n",
        "        stop = (i+1)*self.batch_size\n",
        "        \n",
        "        # PLACEHOLDERS FOR BATCHED DATA\n",
        "        batch_enc =[]\n",
        "        batch_dec_input = []\n",
        "        batch_dec_out =[]\n",
        "\n",
        "        for j in range(start,stop): \n",
        "            \n",
        "            a,b,c = self.dataset[j] \n",
        "            batch_enc.append(a[0]) \n",
        "            batch_dec_input.append(b[0])\n",
        "            batch_dec_out.append(c[0]) \n",
        "        \n",
        "        # Conveting list to array   \n",
        "        batch_enc = (np.array(batch_enc)) \n",
        "        batch_dec_input = np.array(batch_dec_input)\n",
        "        batch_dec_out = np.array(batch_dec_out)\n",
        "        \n",
        "        ## RETURNING BATCHED DATA IN REQUIRED FORM\n",
        "        return [batch_enc , batch_dec_input],batch_dec_out\n",
        "    def __len__(self):\n",
        "        # Returning the number of batches\n",
        "        return int(self.totl_points/self.batch_size) "
      ],
      "metadata": {
        "id": "8Jpmkhbp9T8t"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def encoder(input_shape,vocab, emb_output, lstm_units, enc_input):\n",
        "    '''THIS FUNCTION TAKES IN THE SEQUENCES AND RETURNS THE ENCODER OUTPUT'''\n",
        "    ## FIRST LAYER : EMBEDDING LAYER\n",
        "    enc_emb = layers.Embedding(vocab, emb_output,mask_zero = True,input_length=input_shape)(enc_input)\n",
        "    ## SECOND LAYER : LSTM LAYER\n",
        "    enc_lstm , enc_state_h,enc_state_c = layers.LSTM(units= lstm_units,return_sequences=True,return_state=True)(enc_emb)\n",
        "    ## RETURNING THE LSTM OUTPUTS AND STATES\n",
        "    return enc_lstm , enc_state_h,enc_state_c\n",
        "\n",
        "\n",
        "## DEFINING THE DECODER LAYER AS A FUNCTION \n",
        "def decoder(input_shape,vocab, emb_output, lstm_units,enc_states, dec_input):\n",
        "    ## FIRST LAYER : EMBEDDING LAYER\n",
        "    dec_emb = layers.Embedding(vocab, emb_output , mask_zero = True,input_length=input_shape)(dec_input)\n",
        "    ## SECONG LAYER : LSTM LAYER\n",
        "    dec_lstm, dec_state_h,dec_state_c = layers.LSTM(units=lstm_units,return_sequences=True,return_state=True)(dec_emb,initial_state= enc_states)\n",
        "    ## RETURNING THE LSTM OUTPUTS AND STATES\n",
        "    return dec_lstm, dec_state_h,dec_state_c\n",
        "  "
      ],
      "metadata": {
        "id": "keKwFfEU-FOD"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Attention(tf.keras.layers.Layer):\n",
        "  \n",
        "    '''THIS FUNCTION RETURNS THE CONTEXT VECTOR AND ATTENTION WEIGHTS (ALPHA VALUES)'''\n",
        "    def __init__(self,units):\n",
        "        super().__init__()\n",
        "        # INITIALIZING THE NUMBER OF UNITS IN ATTENTION MODEL\n",
        "        self.units =  units\n",
        "\n",
        "    def call(self,enc_output,dec_state):\n",
        "        # EXPANDING THE DIMENSION OF DECODER STATE  EG. FROM (16,32) TO (16,32,1)\n",
        "        dec_state =  tf.expand_dims(dec_state,axis=-1)\n",
        "        \n",
        "        # MATRIX MULTIPLICATION OF ENCODER OUTPUT AND MODIFIED DECODER STATE\n",
        "        # (16,32,1)*(16,13,32) = (16,13,1)\n",
        "        score = tf.matmul(enc_output,dec_state)\n",
        "        \n",
        "        # APPLYING SOFTMAX TO THE AXIS 1\n",
        "        # OUPUT SHAPE = (16,13,1)\n",
        "        att_weights = tf.nn.softmax(score,axis=1)\n",
        "        \n",
        "        # CALCULATING THE CONTEXT VECTOR BY FIRST ELEMENTWISE MULTIPLICATION AND THEN ADDING THE AXIS 1\n",
        "        # (16,13,1)*(16,13,32)=(16,13,32)\n",
        "        context_vec  = att_weights* enc_output\n",
        "        # (16,13,32) SUM AND REDUCE THE DIMENSION AT AXIS 1 => (16,32)\n",
        "        context_vec = tf.reduce_sum(context_vec,axis=1)\n",
        "        \n",
        "        # RETURNING THE CONTEXT VECTOR AND ATTENTION WEIGHTS\n",
        "        return context_vec,att_weights\n"
      ],
      "metadata": {
        "id": "h5054faKG5Rl"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Monotonic_Attention(tf.keras.layers.Layer):\n",
        "    '''THIS FUNCTION RETURNS THE CONTEXT VECTOR AND ATTENTION WEIGHTS (ALPHA VALUES)'''\n",
        "    def __init__(self,units,att_mode):\n",
        "        super().__init__()\n",
        "        # INITIALIZING THE DENSE LAYER W1\n",
        "        self.W1 = layers.Dense(units)\n",
        "        # INITIALIZING THE DENSE LAYER W2\n",
        "        self.W2 = layers.Dense(units)\n",
        "        # INITIALIZING THE DENSE LAYER V\n",
        "        self.v = layers.Dense(1)\n",
        "        self.mode = att_mode\n",
        "        \n",
        "    def call(self,enc_output,dec_state,prev_att):\n",
        "        # HERE WE ARE COMPUTING THE SCORE \n",
        "\n",
        "        if self.mode == \"dot\":\n",
        "        # FINDING THE SCORE FOR DOT MODEL\n",
        "            dec_state =  tf.expand_dims(dec_state,axis=-1)\n",
        "            score = tf.matmul(enc_output,dec_state)\n",
        "            score = tf.squeeze(score, [2])\n",
        "            \n",
        "            \n",
        "        if self.mode == \"general\":\n",
        "        # FINDING THE SCORE FOR GENERAL MODEL\n",
        "            dec_state =  tf.expand_dims(dec_state,axis=-1)\n",
        "            dense_output = self.W1(enc_output)\n",
        "            score = tf.matmul(dense_output , dec_state)\n",
        "            score = tf.squeeze(score, [2])\n",
        "            \n",
        "            \n",
        "        if self.mode == \"concat\":\n",
        "        # FINDING THE SCORE FOR CONCAT MODEL\n",
        "            dec_state =  tf.expand_dims(dec_state,axis=1)\n",
        "            score = self.v(tf.nn.tanh(\n",
        "                self.W1(dec_state)+ self.W2(enc_output)))\n",
        "            score = tf.squeeze(score, [2])\n",
        "        \n",
        "        # AFTER THE SOCRES ARE COMPUTED THE SIGMOID IS USED ON IT\n",
        "        probabilities = tf.sigmoid(score)\n",
        "\n",
        "        # ATTENTION WEIGHTS FOR PRESENT TIME STEP\n",
        "        probabilities = probabilities*tf.cumsum(tf.squeeze(prev_att,-1), axis=1)\n",
        "        attention = probabilities*tf.math.cumprod(1-probabilities, axis=1, exclusive=True)\n",
        "        attention = tf.expand_dims(attention,axis=-1)\n",
        "        \n",
        "        # CONTEXT VECTOR\n",
        "        context_vec  =  attention  * enc_output\n",
        "        context_vec = tf.reduce_sum(context_vec,axis=1)\n",
        "        \n",
        "        # RETURN CONTEXT VECTOR AND ATTENTION\n",
        "        return context_vec, attention"
      ],
      "metadata": {
        "id": "JWSELuQPHCjh"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(ita_text,model):\n",
        "    '''THIS FUNCTION IS USED IN INFERENCE TIME WHICH GIVEN ANY SENTENCE IN ITALIAN OUTPUTS THE ENGLISH SENTENCE AND ALPHA VALUES'''\n",
        "    # FORMING TOKENIZED SEQUENCES FOR INPUT SENTENCE\n",
        "    seq = tk_inp.texts_to_sequences([ita_text])\n",
        "    # PADDING THE SEQUENCES\n",
        "    seq = pad_sequences(seq,maxlen = 20 , padding=\"post\")\n",
        "    # INITIALIZING THE STATES FOR INPUTING TO ENCODER\n",
        "    state = model.layers[0].initialize(1)\n",
        "    # GETTING THE ENCODED OUTPUT\n",
        "    enc_output,state_h,state_c= model.layers[0](seq,state)\n",
        "    # VARIABLE TO STORE PREDICTED SENTENCE\n",
        "    pred = []\n",
        "    # THIS VARIABLE STORES THE STATE TO BE INPUTED TO ONE STEP ENCODER\n",
        "    input_state_h = state_h\n",
        "    input_state_c = state_c\n",
        "    # THIS VARIABLE STORES THE VECTOR TO VE INPUTED TO ONE STEP ENCODER\n",
        "    current_vec = tf.ones((1,1))\n",
        "    # THIS VARIABLE WILL STORE ALL THE ALPHA VALUES OUTPUTS\n",
        "    alpha_values = []\n",
        "\n",
        "    for i in range(20):\n",
        "        # PASSING THE REQUIRED VARIABLE TO ONE STEP ENCODER LAYER\n",
        "        fc , dec_state_h ,dec_state_c, alphas = model.layers[1].layers[0](enc_output , current_vec ,input_state_h ,input_state_c)\n",
        "        #APPENDING THE ALPHA VALUES TO THE LIST \"alpha_values\"\n",
        "        alpha_values.append(alphas)\n",
        "         # UPDATING THE CURRENT VECTOR \n",
        "        current_vec = np.argmax(fc , axis = -1)\n",
        "         # UPDATING THE INPUT STATE\n",
        "        input_state_h = dec_state_h\n",
        "        input_state_c = dec_state_c\n",
        "        # GETTING THE ACTUAL WORDS FRO THE TOKENIZED INDEXES\n",
        "        pred.append(tk_out.index_word[current_vec[0][0]])\n",
        "        # IF THE WORD \"<end>\" COMES THE LOOP WILL BREAK\n",
        "        if tk_out.index_word[current_vec[0][0]]==\"<end>\":\n",
        "              break\n",
        "    # JOINING THE PREDICTED WORDS\n",
        "    pred_sent = \" \".join(pred)\n",
        "    # CONCATINATING ALL THE ALPHA VALUES\n",
        "    alpha_values = tf.squeeze(tf.concat(alpha_values,axis=-1),axis=0)\n",
        "    # RETURNING THE PREDICTED SENTENCE AND ALPHA VALUES\n",
        "    return  pred_sent , alpha_values"
      ],
      "metadata": {
        "id": "6g2AujkjHZYq"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def beam_search(input,model,k):\n",
        "    seq = tk_inp.texts_to_sequences([input])\n",
        "    seq = pad_sequences(seq,maxlen = 35,padding=\"post\")\n",
        "\n",
        "    state = model.layers[0].initialize(1)\n",
        "    # GETTING THE ENCODED OUTPUT\n",
        "    enc_output,enc_state_h,enc_state_c = model.layers[0](seq,state)\n",
        "    \n",
        "\n",
        "    input_state_h = enc_state_h\n",
        "    input_state_c = enc_state_c \n",
        "    k_beams = [[tf.ones((1,1),dtype=tf.int32),0.0]]\n",
        "    for i in range(35):\n",
        "        candidates = []\n",
        "        for sent_pred , prob in k_beams :\n",
        "            if tk_out.word_index[\"<end>\"] in sent_pred.numpy() :\n",
        "                candidates += [[sent_pred , prob]]\n",
        "            else:\n",
        "               \n",
        "                dec_input = model.layers[1].layers[0].layers[0](sent_pred)\n",
        "                dec_output , dec_state_h , dec_state_c   =  model.layers[1].layers[0].layers[2](dec_input ,  initial_state =  [input_state_h , input_state_c])\n",
        "\n",
        "                context_vec , alphas =  model.layers[1].layers[0].layers[1](enc_output,dec_state_h)\n",
        "\n",
        "                # CONCATINATING THE CONTEXT VECTOR(BY EXPANDING DIMENSION) AND ENBEDDED VECTOR\n",
        "                dense_input =  tf.concat([tf.expand_dims(context_vec,1),tf.expand_dims(dec_state_h,1)],axis=-1)\n",
        "                \n",
        "                # PASSING THE DECODER OUTPUT THROUGH DENSE LAYER WITH UNITS EQUAL TO VOCAB SIZE\n",
        "                dense = model.layers[1].layers[0].layers[3](dense_input)\n",
        "\n",
        "                pred = tf.argsort(dense, direction= 'DESCENDING')[:,:,:k]\n",
        "                for w in range(k):\n",
        "                  candidates += [[tf.concat((sent_pred, pred[:,:,w]) , axis=-1) , (prob + tf.math.log(dense[:,:,pred[:,:,w][0][0]])[0][0])]  ]\n",
        "        k_beams = sorted(candidates,key=lambda tup:tup[1],reverse=True)[:k]\n",
        "\n",
        "    all_sent = []\n",
        "    for i,score in k_beams:\n",
        "        sent = \"\"\n",
        "        for j in range(1,35):\n",
        "            sent +=  tk_out.index_word[i.numpy()[:,j][0]] +  \" \" \n",
        "            if tk_out.index_word[i.numpy()[:,j][0]] ==\"<end>\":\n",
        "                break\n",
        "        all_sent.append((sent.strip(),score.numpy()))\n",
        "    return all_sent"
      ],
      "metadata": {
        "id": "Uo8tktU-Hxpq"
      },
      "execution_count": 43,
      "outputs": []
    }
  ]
}